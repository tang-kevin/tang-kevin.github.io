@InProceedings{JeongSchmitzKakoluRSteinTang_LDL_SIGMORPHON_2023,
  author    = {Cheonkam Jeong and Dominic Schmitz and Akhilesh {Kakolu Ramarao} and Anna Sophia {Stein} and Kevin Tang},
  title     = {Linear {D}iscriminative {L}earning: a competitive non-neural baseline for morphological inflection},
  booktitle = {Proceedings of the 20th {SIGMORPHON} Workshop on {C}omputational {R}esearch in {P}honetics, {P}honology, and {M}orphology},
  year      = {{To appear}},
  note      = {(Equal contribution authorship: CJ, DS and AKR; Senior and corresponding author: KT) Preprint: \url{https://psyarxiv.com/u6sv4}},
  publisher = {Association for Computational Linguistics},
  abstract  = {This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively Plausible Morphophonological Generalization in Korean. We implemented both Linear Discriminative Learning and Transformer models and found that the Linear Discriminative Learning model trained on a combination of corpus and experimental data showed the best performance with the overall accuracy of around 83\%. We found that the best model must be trained on both corpus data and the experimental data of one particular participant. Our examination of speaker-variability and speaker-specific information did not explain why a particular participant combined well with the corpus data. We recommend Linear Discriminative Learning models as a future non-neural baseline system, owning to its training speed, accuracy, model interpretability and cognitive plausibility. In order to improve the model performance, we suggest using bigger data and/or performing data augmentation and incorporating speaker- and item-specifics considerably.},
}